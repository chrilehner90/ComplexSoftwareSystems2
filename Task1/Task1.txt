Task 1


1.1.
====


Processor time (CPU Time):
Processor time (CPU Time) ist die Zeit, die der Prozessor tatsächlich um Ausführen von Instruktionen in einem Programm braucht. Die CPU time teilt sich dabei in verschiedene (Sub-)Zeiten auf. Diese Zeiten sind Systemtime, Usertime, Idletime, Stealtime und 

Ausführungszeit messen:\newline
- mit Funktionen (time(), clock_gettime(), clock(), rdtsc)



1.3.
====


Hardware: 
---------
	- persistenter Speicher: 32 * 1 TB SSD = 32TB (Samsung MZ-75E1T0B/EU EVO 850 interne SSD 1TB)
	- Datendurchsatz pro SSD: 6 Gbit/s = 750 MB/s
	- Festplattenanordnung: Raid 0 mit 32 1TB SSDs
	- RAM: gesamt 64 GB RAM; Typ: DDR3 SDRAM, 17066 MB/s = ~17 GB/s (http://bit.ly/1EanwjI)
	- Netzwerk: Glasfaser (10 Gbit/s = 1,25 GB/s)
	- CPU: Intel Xeon E5-2697 v3

E-Mail Adressen (Annahmen):
---------------------------
	- Median der Länge von E-Mail Adressen: 19 Zeichen (http://bit.ly/1AxsP5i)
	- ASCII Codierung: 7 Bit pro Zeichen
	- 19 Zeichen * 7 Bit = 133 Bit = 16,625 Byte pro Adresse
	- 32 TB = 32000000000000 / 16,625 = 1.924.812.030.075,2 Adressen = ~ 2 Billionen Adressen


Übertragung über das Netzwerk:
------------------------------
	32TB / 1,25GB/s = 32 000 000 000 000 Bytes / 1 250 000 000 Bytes/s = 3 200 000 / 125 = 25600 sec = 7,11 Stunden


Sortierung:
-----------
	Vorsortierung:
		- Die 32 TB Daten in 1000 mal 32 GB Chunks aufteilen (32GB um bei der eigentlichen Sortierung den halben Arbeitsspeicher zu benutzen, damit bleiben noch ~32GB für den Output Buffer)
		- Jeden dieser Chunks mit einem parallelen Mergesort mit Laufzeit O(log n) sortieren und in jeweils eine Datei schreiben (Das Ergebnis hierbei sind 1000 Dateien mit jeweils 32GB --> Chunks)

	Sortierverfahren: Externer (paralleler) Mergesort
		- Aus jedem der 1000 sortierten Chunks mit jeweils 32 GB den obersten Teil herausnehmen und in je einen Input Buffer schreiben (32GB / 1000 = 32MB pro Input Buffer)
		- Mit dem merge Schritt wieder sortiert zusammenfügen und in den Output Buffer schreiben
		- Sobald der Output Buffer voll ist, wird er in eine Datei geschrieben, falls diese noch nicht existiert. Falls doch, wird er unten angehängt. Der Output Buffer wird dann geleert.
		- Sobald ein Input Buffer leer ist, wird er mit Daten aus dem entsprechenden Chunk aufgefüllt
		- Ergebnis dieses Vorgangs ist eine 32 TB große Datei mit sortierten Daten

Resultat:
---------
	Lesen/Schreiben:
		- pro Datei ~43 Sekunden (32GB pro Datei / 750 MB/sek)
		- 1000 * 43 Sekunden = 43000 Sekunden = ~11.95 Stunden

	Sortieren:
		
